---
layout: post
title: Data Science Bootcamp, Metis - Project_03
---
<style>
img {
 display: block;
 max-width: 100%;
 margin: 0 0 1rem;
 border-radius: 5px;
 margin-left: auto;
  margin-right: auto
}
p {
font-size: small;
}

# Introduction

The length of stay in a hotpital for a patient depends on many factors. This makes estimating the length of stay for a particular patient a difficult task for hospitals, and the estimation differs for different hospitals. Because of that, health care providers have a hard time deciding whether a particular hospital is overcharging them by keeping the patient hospitalized for too long. In this project, I attempt to build a classification model that predicts the appropriate length of stay period for each patient using the data collected about that patient during their stay in the hospital. The actual length of stay period is then compared to the predicted one to determine whether the hospital is overcharging the health care provider or not.


![Hospital]({{ site.url }}/images/proj03/hospital.jpg)

# Steps

## Data Acquisition
Data was collected from [MIMIC Critical Care Database](https://mimic.physionet.org). MIMIC is an openly available dataset developed by the MIT Lab for Computational Physiology. The dataset consists of deidentified health data associated with around 60,000 hospital admissions. It includes some personal information about the patients such as age, gender, and religion, as well as some information regarding thier stay at the hospital such as the number of times they called the nurse, and number of lab tests that was performed on them.

![MIMIC]({{ site.url }}/images/proj03/MIMIC.png)

## Data Cleaning
This step started with constructing raw dataFrame that needs to be cleaned and organized.

There is a column indicating whether the patient died in the hospital or not. There were 5854 cases where the hospitalized individuals passed away during their stay. These data points were removed from the dataset becasue they do not serve the purpose of the project and might worsen the performance of the final model. After that, the entire column was removed from the dataFrame as all of its values were the same.

Most of the categorical features were already cleaned and ready for modeling except for 'religion', 'ethnicity', 'marital_status', 'AdmitDiagnosis', and 'AdmitProcedure'.

When it comes to 'religion', there were 20 unique values with Catholics being the majority.

![Religion before]({{ site.url }}/images/proj03/proj03_religion_dist_01.png)

These different values for 'religion' were gathered into three categories.

![Religion after]({{ site.url }}/images/proj03/proj03_religion_dist_02.png)

Looking at the unique values of the 'ethnicity' column, many of them could be grouped into one category (for example, 'Asian - Chinees' grouped with 'Asian').

![Ethnicity before]({{ site.url }}/images/proj03/proj03_ethnicity_dist_01.png)

The 'ethnicity' column was modified to have a total of 6 unique values.

![Ethnicity after]({{ site.url }}/images/proj03/proj03_ethnicity_dist_02.png)

When it comes to the 'marital_status', it was too specific for the purpose of the project.

![Marital status before]({{ site.url }}/images/proj03/proj03_maritalStatus_dist_01.png)

The column values were transformed to indicate whether that patient is emtionally attatched to someone or not.

![Marital status after]({{ site.url }}/images/proj03/proj03_maritalStatus_dist_02.png)

As far as the 'AdmitDiagnosis', and 'AdmitProcedure' columns, there are phrases and sentences in the form of strings with over 12,000 unique values. For this reason, label incoding is considered later on.

Perhaps the most important part of this step is regarding dividing the target into classes. The target of the model was a numerical value in days for the length of stay in the hospital. In order to decide where to draw the line for each class, a histogram for the distribution of the target was ploted.

![LOS distribution]({{ site.url }}/images/proj03/proj03_LOSdays_dist_02.png)

After that, the target was classified into 4 different classes.

![LOSgroup distribution]({{ site.url }}/images/proj03/proj03_LOSgroup_dist.png)

Finally, the columns were organized and renamed. Now the dataFrame is ready for building models.

## Classification Models
Two dataFrames were formed, one using dummy variables and another with label encoding.

- knn (k = 8)

![knn]({{ site.url }}/images/proj03/proj03_knn.png)

- Logistic (c = 10)

![Logistic]({{ site.url }}/images/proj03/proj03_logistic.png)

- Random Forest (n_entimators = 30, dummy_depth = 16, lableEncoding_depth = 22)

For the random forest, I have built 40 forests with n_estimators ranging from 1 to 40 and recorded the variance and bias for all.

![Variance]({{ site.url }}/images/proj03/proj03_vatiance_vs_estimators.png)

![Bias]({{ site.url }}/images/proj03/proj03_bias_vs_estimators.png)

From the previous two graphs, increasing the number of trees in the random forest to 30 is a good tradeoff between complexity and variance, while bias is somewhat stable.

![Random forest]({{ site.url }}/images/proj03/proj03_randomForest.png)

At the end, the random forest model was choosing for the final test.
  
# Test

The choosen model was tested and scored 0.9171

![Confusion matrix]({{ site.url }}/images/proj03/proj03_confusion_matix.png)

Looking at the confusion matrix, when the model misclassified a point, it did not misclassified it by a range of more than one class.

![Precision_vs_Recall]({{ site.url }}/images/proj03/proj03_precision_vs_recall.png)


# Future Work

Text analysis would be perfomred on the 'AdmitDiagnosis', and 'AdmitProcedure' columns.

![AdmitDiag dictionary]({{ site.url }}/images/proj03/proj03_admitDiag_dict.png)

![AdmitProc dictionary]({{ site.url }}/images/proj03/proj03_admitProc_dict.png)
